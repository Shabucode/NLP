{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQzrSsSzjELY"
      },
      "source": [
        "# **### Emotion Label Detection On GoEmotion Simplified Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h6npvP5j3q_",
        "outputId": "5240e6b7-d6db-4d95-a9fb-613a5a4b50b9"
      },
      "outputs": [],
      "source": [
        "pip install datasets #install datasets for loading GoEmotion Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlkAYkAXlDRe"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856,
          "referenced_widgets": [
            "4a7f4a8c09fb4b7380c6b62860fd5dd9",
            "e8c6397072de48bbaff4c1986805c3e5",
            "50006f604fcb4874a266146cbf169611",
            "29b50964b191480eab23ef110dbb196a",
            "ecdaa489abc0415d8648f40d7d591224",
            "3be0d5adfc7045ad896812466c9d8778",
            "2d747be369134dfb8da53c1f3f219373",
            "f00082458f1e4dcdbaf36aa18d546177",
            "80f3e64807724c5fb881660218a4a083",
            "9a961937e74b4f84869ad2f37084b860",
            "bf841d8aa633405cbf34d406b543f31c",
            "cc7b04401dea4e19aa55e51ebdaf2bf7",
            "6b3a42ea0c534843a59c0ccbded33515",
            "fd9868af3318469984ab9e8e3c2b1f96",
            "622bc2a391564b3b95c60b488cbcac75",
            "6eece97a872b4b929689cbca0d098b5a",
            "ce784db527434095b1fecacbb676cf9f",
            "8fc868f2cbd446bbb41c710cdae67414",
            "5eca2476330a497690f23c93f728eb12",
            "17f4bb83827c43f584b60b40a023e290",
            "3bb3aba5984441849f8b0cda9934f5ac",
            "37644b7a554a4b61bc244d27c1c471d3",
            "fb6c747e519646ef9623b63741b34f4d",
            "e2ced9fb7a574ede99bf3ce9e5dac42e",
            "51f7bdc5abc341cb85e9643135674ad1",
            "5f5d497cf70945bdb730ef87b7e95cb5",
            "526e403cbd984228a9cb8347663d791f",
            "83044af48937456984b495341c3633d7",
            "297530bb0b614c36bd2f6ab7b02b8808",
            "f6f5e6ed1bec4dae9f64d8b6497ff11a",
            "9d445f644b90461fa50758c7fe4c302e",
            "519d92f136aa4d7782084a33475a8ab6",
            "49c4326299854853b05c7686b9dc8c8e",
            "3b458d6d82f74cfbae2cbf263811d369",
            "4b56728825994a90ba0f99a6a82dd620",
            "2883a301d7ca46c287b2d2e94aebb311",
            "0893c089762b4bb7ae23359da3327881",
            "2c9aef20f72a45ca8359e0b2ffe87d9e",
            "e8e9cac3d4224b7d941cba8d43349fa0",
            "36627f4f9d104e7188e090c6d140cc4f",
            "7390e1b6eafd45cf9b14131ff71ab63f",
            "2f099122342e4ea9934632981908765b",
            "b36b89b3db434739b717d9626bc17b4a",
            "d32543c837ab4618a2b6a93fa2c01aac",
            "7103d302f6e14193905226c7fcc76787",
            "c5a32fa3fb8b4ed9820530feb23b0d70",
            "ac2d704fdf854f5083e421cc22b9de34",
            "923a489b8fb94b5290b4e980736f536d",
            "10e670c34aee458ab3e2ea1418cb1960",
            "d799a096a1b741f6ab3589396ff186a9",
            "daac631cb9214b95bd1f3fe825bac5a1",
            "02f6c8af77874770ae9fde62d5450a09",
            "f59d83ce1a6c4476ba274e9b3920fc78",
            "14b6f618eb444351807b2744c7df142b",
            "9fec5930320a4749b1baca82c3205126",
            "b3c87e9da5684289828d0cb0e61727ea",
            "3243eb14082d4bf5b23a70a00c3c7517",
            "2e7a5ad290684214aa434d3a80c9952e",
            "d104ceea9d9f49d782cdeea6a08d6081",
            "1fa51d5753e748b9afafb4c2c0ab0480",
            "1168031e344345b8baab783c699f1534",
            "5a160ff37f4745149d4a3d83c4e4e9bc",
            "955694989c9946be850de641745f2698",
            "474e3e479b3043ffa0175d5f5118210c",
            "b82c7b3471e94f37a5464f8daca413e3",
            "e0792081b6c8497fb55d8bc3c90885a5",
            "432c2b87e35744699c554f515c3655f8",
            "5a28beb8d5c640c5a54086b2203af491",
            "f9797f7e981d4828b583d36f11ffb107",
            "b6b9cd1a94ef44679a25bb5ecaf16892",
            "18663eecb3aa4dd58e15fabfcb15be61",
            "8f1a0d56970b4c648169d04dda176b00",
            "0ecf561c33c34c7d89c3654514dddb01",
            "a6f2e448d13244afbba55e1d6dd7810d",
            "03fe034a7d6643938015cdd79063d172",
            "170321e043c449b79c948a9b761b9455",
            "e0667fe462b44df19410b0dd831048a6",
            "87a046bd1781405a81c0a692ca53c60e",
            "3a513f6a0d904fbfae30163b7e385bf3",
            "c521a9d59bca4b3da215148937063da6",
            "e38da3d5e3304120ad6fd4a12546792b",
            "a63fcea745ec4764a8034ef706c8a1a3",
            "220e8a2fd39e4f0090adf734822035ff",
            "4be29f4861ef4223a359f49cdbf4aef8",
            "e106d2c76f7e4dbeb776a0b4522b0c04",
            "3865f53165d14ca0a6099bfbf4f90765",
            "b5dc65939c7544829ebd94b571139304",
            "c354c896d59c49d6ad982d6bb1d43eef",
            "7f19333689d64d6597a0b406dfe5c28a",
            "cafa815cdf4f4f13b1ece5b8154f3de0",
            "97ce7c6c37cf4c859ea5bb6a6f0a934d",
            "6109f00532914ee99226bc616956342f",
            "89a05f5e038c4056b5a2ad9430b24977",
            "ffbde375c68d405cbc0d2a6622caf03b",
            "630c4c2c40004444b8d49c0711bee60a",
            "481a2184355543c8a53c28f6c610855b",
            "a43c887367dc4c37911ca34b16d5b91e",
            "80361592cc5e4555a5524575c54b02e6",
            "e2a099d27daf46af979173234636fb4c",
            "4caa7a0108944ccdbfd7d0c8393ae94a",
            "c4f626f3f8854f768ba458f60de77c73",
            "a1a4413692dd4b87b9fa27a870750620",
            "39f716f4395442dbbbee1f0b1083a999",
            "09148c9d63dc41c4862ec2ad8f405071",
            "042426b169ec4779b4308c0fa3c38c31",
            "db39cb1276e8406182afd4d30517cab9",
            "e9535f61a7b5448d895dd68a28aa1213",
            "d0e3e5f5b4d94ec7854bee42c44f3f34",
            "13eaf3ecddb4440a94e801e16d4a4043",
            "23c207a7f4ea4d8fa8b433c0fba58601"
          ]
        },
        "id": "1i9G-srMwUP-",
        "outputId": "dbb239f7-22de-44f0-f83c-8a50ad4515f6"
      },
      "outputs": [],
      "source": [
        "#Importing necessary Libraries\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "go_emotion_simplified = load_dataset('go_emotions')#loading dataset GoEmotion\n",
        "# go_emotion_simplified_train = pd.DataFrame(go_emotion_simplified['train'])   ## While execution took more time\n",
        "go_emotion_simplified_train = go_emotion_simplified['train'].to_pandas() ## Lesser execution time\n",
        "go_emotion_simplified_validation = go_emotion_simplified['validation'].to_pandas()\n",
        "go_emotion_simplified_test = go_emotion_simplified['test'].to_pandas()\n",
        "\n",
        "go_emotion_simplified_all = pd.concat([go_emotion_simplified_train, go_emotion_simplified_validation , go_emotion_simplified_test], ignore_index=True)\n",
        "# Number of labels assigned to text\n",
        "go_emotion_simplified_all['labels_count'] = go_emotion_simplified_all['labels'].apply(lambda x: len(x))\n",
        "# Get the labels name\n",
        "\n",
        "text_labels = \"\"\" 0: admiration\n",
        "        1: amusement\n",
        "        2: anger\n",
        "        3: annoyance\n",
        "        4: approval\n",
        "        5: caring\n",
        "        6: confusion\n",
        "        7: curiosity\n",
        "        8: desire\n",
        "        9: disappointment\n",
        "        10: disapproval\n",
        "        11: disgust\n",
        "        12: embarrassment\n",
        "        13: excitement\n",
        "        14: fear\n",
        "        15: gratitude\n",
        "        16: grief\n",
        "        17: joy\n",
        "        18: love\n",
        "        19: nervousness\n",
        "        20: optimism\n",
        "        21: pride\n",
        "        22: realization\n",
        "        23: relief\n",
        "        24: remorse\n",
        "        25: sadness \n",
        "        26: surprise\n",
        "        27: neutral\"\"\"\n",
        "\n",
        "labels_dict = dict()\n",
        "\n",
        "# labels_dict[int(kval.split(':')[0].strip())] = kval.split(':')[-1].strip()\n",
        "\n",
        "for f in [kval for kval in [kv for kv in text_labels.split(\"\\n\")]]:\n",
        "    labels_dict[int(f.split(':')[0].strip())] = f.split(':')[-1].strip()\n",
        "\n",
        "for class_name in labels_dict:\n",
        "    go_emotion_simplified_all[labels_dict[class_name]] = go_emotion_simplified_all['labels'].apply(lambda x: 1 if class_name in x else 0) # Since we have numpy arrays in labels column\n",
        "\n",
        "\n",
        "\n",
        "go_emotion_simplified_all.head(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78zUhm7iCy-o"
      },
      "source": [
        "#Merging 27 emotions into 13 emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMT-Ftk0zha1",
        "outputId": "22ad3cfa-56e4-4253-83d9-0e9e55c155b7"
      },
      "outputs": [],
      "source": [
        "# merge columns to create new categories\n",
        "go_emotion_simplified_all['happiness'] = go_emotion_simplified_all['joy'] + go_emotion_simplified_all['amusement']\n",
        "go_emotion_simplified_all['sadness'] = go_emotion_simplified_all['grief'] + go_emotion_simplified_all['sadness']\n",
        "go_emotion_simplified_all['fear'] = go_emotion_simplified_all['fear'] + go_emotion_simplified_all['confusion'] +go_emotion_simplified_all['nervousness']\n",
        "go_emotion_simplified_all['surprise'] = go_emotion_simplified_all['curiosity'] + go_emotion_simplified_all['surprise']\n",
        "go_emotion_simplified_all['anger'] = go_emotion_simplified_all['anger'] + go_emotion_simplified_all['annoyance']\n",
        "go_emotion_simplified_all['disgust'] = go_emotion_simplified_all['disappointment'] + go_emotion_simplified_all['disgust'] + go_emotion_simplified_all['disapproval']\n",
        "go_emotion_simplified_all['anticipation'] = go_emotion_simplified_all['excitement'] + go_emotion_simplified_all['optimism']\n",
        "go_emotion_simplified_all['realization'] = go_emotion_simplified_all['realization'] + go_emotion_simplified_all['pride']\n",
        "go_emotion_simplified_all['desire'] = go_emotion_simplified_all['admiration'] + go_emotion_simplified_all['desire']\n",
        "go_emotion_simplified_all['shame'] = go_emotion_simplified_all['embarrassment'] + go_emotion_simplified_all['remorse']\n",
        "go_emotion_simplified_all['relief'] = go_emotion_simplified_all['relief'] + go_emotion_simplified_all['gratitude']\n",
        "go_emotion_simplified_all['love'] = go_emotion_simplified_all['love'] + go_emotion_simplified_all['caring']\n",
        "\n",
        "\n",
        "# keep desired columns and drop original columns\n",
        "go_emotion_simplified_all = go_emotion_simplified_all[['text', 'happiness', 'sadness', 'fear', 'surprise', 'anger', 'disgust', 'approval','anticipation', 'realization','desire','shame','relief','love','neutral']]\n",
        "Emotion_labels = go_emotion_simplified_all[['happiness', 'sadness', 'fear', 'surprise', 'anger', 'disgust', 'approval','anticipation', 'realization','desire','shame','relief','love','neutral']]\n",
        "text = go_emotion_simplified_all[['text']]\n",
        "# view the resulting dataframe\n",
        "print(go_emotion_simplified_all.head(20))\n",
        "print(Emotion_labels.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH9_B22OkKg1"
      },
      "source": [
        "### **Data Analysis And Visualisation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn86wGCD_GlT"
      },
      "source": [
        "## **Bar Chart** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "yp9n3gnZBBoC",
        "outputId": "e2bdd2c0-1f40-4f05-e591-791760173fcf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Get the sum of each emotion column\n",
        "emotion_counts = go_emotion_simplified_all.iloc[:, 1:].sum()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(emotion_counts.index, emotion_counts.values)\n",
        "\n",
        "# Set the title and labels for the plot\n",
        "plt.title('Frequency of Emotion Categories')\n",
        "plt.xlabel('Emotion Category')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Rotate the x-axis labels to avoid overlap\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tttiQRl__Cyq"
      },
      "source": [
        "#Scatter Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "x_ikWfH34vxq",
        "outputId": "59670c08-4cdf-4576-9c5e-e498a8f43ed9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        " # Define colors for each emotion\n",
        "colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan', 'magenta', 'yellow', 'grey', 'purple']\n",
        "\n",
        "# Extract the desired emotions\n",
        "emotions = ['happiness', 'sadness', 'fear', 'anger', 'surprise', 'disgust', 'approval', 'anticipation', 'realization', 'desire', 'shame', 'relief', 'love','neutral']\n",
        "\n",
        "# Create a scatter plot of the emotions\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for i, emotion in enumerate(emotions):\n",
        "    ax.scatter(go_emotion_simplified_all.index, go_emotion_simplified_all[emotion], label=emotion, color=colors[i])\n",
        "\n",
        "ax.legend()\n",
        "ax.set_xlabel('Sample')\n",
        "ax.set_ylabel('Emotion score')\n",
        "ax.set_title('Emotion Scores in GoEmotions Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ws4Dv7DMpV"
      },
      "source": [
        "#Pie Chart for emotions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "5rAK1Fjt8V21",
        "outputId": "8ccf1401-e5f9-4b2b-d8fb-448a79666cba"
      },
      "outputs": [],
      "source": [
        "# Create a pie chart of the overall emotion frequencies\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.pie(emotion_counts.values, labels=emotion_counts.index, autopct='%1.1f%%')\n",
        "plt.title('Frequency of Emotions in GoEmotions Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_TBnn_PFkYT"
      },
      "source": [
        "### **#Word Cloud**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "GuGmVxuRDsxe",
        "outputId": "9172dd04-e26a-4180-8bde-e1dda97c145e"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud matplotlib pandas\n",
        "text_data = go_emotion_simplified_all['text'].values\n",
        "text = ' '.join(text_data)\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud().generate(text)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n",
        "print(text_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "5GBYBxE7GeLE",
        "outputId": "97b5a8e0-a7aa-4057-8df6-90e1372f597b"
      },
      "outputs": [],
      "source": [
        "# Create a heatmap of the correlation matrix for the emotions\n",
        "corr = go_emotion_simplified_all[emotions].corr()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation between Emotions in GoEmotions Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz5n7LglgSOC",
        "outputId": "99ff17c3-e860-48e9-b197-e5e5d45928d1"
      },
      "outputs": [],
      "source": [
        "#Length of the text \n",
        "go_emotion_simplified_all['text_len'] = go_emotion_simplified_all['text'].apply(lambda x: len(x.split()))\n",
        "print(go_emotion_simplified_all['text_len'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPPnzVLxj9A9",
        "outputId": "953fe046-f9de-45c8-bbab-c9ede5766390"
      },
      "outputs": [],
      "source": [
        "# Split the text into individual words and count their frequency\n",
        "word_counts = go_emotion_simplified_all['text'].str.split(expand=True).stack().value_counts()\n",
        "\n",
        "print(word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hbcRWpyfkOyV",
        "outputId": "dfe40af5-6a69-4ae6-f08d-2c5096e426c5"
      },
      "outputs": [],
      "source": [
        "# Plot the word frequency distribution as a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Set the number of words to display in the chart\n",
        "top_n = 50\n",
        "\n",
        "word_counts.head(top_n).plot(kind='bar')\n",
        "plt.title('Top {} Most Frequent Words'.format(top_n))\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "plt.hist(word_counts.values, bins=100, log=True)\n",
        "plt.title('Word Frequency Distribution')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xHxfszjidEu"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwoKCwyolfva",
        "outputId": "16976207-1074-4a13-dcc7-e7d0e7288a76"
      },
      "outputs": [],
      "source": [
        "%pip install numpy sklearn nltk #installing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o7Jwjn2f0jt",
        "outputId": "fb9a4c60-d697-483d-beb7-0e2f344ee270"
      },
      "outputs": [],
      "source": [
        "tokens = text.split()  # splitting based on spaces\n",
        "vocab = sorted(set(tokens))  # sorting and removing duplicates by using set()\n",
        "vocab  # just printing the vocab so we can look at it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKytySf5m0CC",
        "outputId": "886f49a7-589d-48c2-fede-36169edd5c16"
      },
      "outputs": [],
      "source": [
        "#Length of tokens and vocab\n",
        "tokens_len = len(tokens)\n",
        "vocab_len = len(vocab)\n",
        "\n",
        "print(f\"Tokens: {tokens_len}\")\n",
        "print(f\"Vocab: {vocab_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqcC9MNdbIrf",
        "outputId": "eb994ba4-6fee-4840-d0a8-82b54e75dfcd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_special_chars(text):\n",
        "    if isinstance(text, str):\n",
        "        # Replace special characters with spaces\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        # Replace multiple spaces with a single space\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remove leading and trailing spaces\n",
        "        text = text.strip()\n",
        "        return text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "\n",
        "go_emotion_simplified_all['text'] = go_emotion_simplified_all['text'].apply(remove_special_chars)\n",
        "print(go_emotion_simplified_all.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "Clhee-K0pKMF",
        "outputId": "2ab7c4fd-cbc2-4629-90b1-0a33e18699e3"
      },
      "outputs": [],
      "source": [
        "# Count the frequency of each word after removing stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_counts = (go_emotion_simplified_all['text']\n",
        "               .str.lower()\n",
        "               .str.split()\n",
        "               .apply(lambda x: [word for word in x if word not in stop_words])\n",
        "               .explode()\n",
        "               .value_counts())\n",
        "\n",
        "# Set the number of words to display in the chart\n",
        "top_n = 50\n",
        "\n",
        "# Create a bar chart of the top N most frequent words\n",
        "plt.figure(figsize=(10, 6))\n",
        "word_counts.head(top_n).plot(kind='bar')\n",
        "plt.title('Top {} Most Frequent Words (Excluding Stopwords)'.format(top_n))\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
        "\n",
        "print(f\"number of stopwords: {len(sklearn_stop_words)}\")\n",
        "print(sklearn_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgwS4HYNV-xd",
        "outputId": "001b2dc6-98c7-4c33-c137-bb55a89a93b1"
      },
      "outputs": [],
      "source": [
        "#Pos wordnet\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map the POS tag to the first character lemmatize() accepts.\"\"\"\n",
        "\n",
        "    try:  # download nltk's POS tagger if it doesn't exist\n",
        "        nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
        "    except LookupError:\n",
        "        nltk.download(\"averaged_perceptron_tagger\")\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()  # use ntlk's POS tagger on the word\n",
        "\n",
        "    # now we need to convert from nltk to wordnet POS notations (for compatibility reasons)\n",
        "    tag_dict = {\n",
        "        \"J\": wordnet.ADJ,\n",
        "        \"N\": wordnet.NOUN,\n",
        "        \"V\": wordnet.VERB,\n",
        "        \"R\": wordnet.ADV\n",
        "    }\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)  # return and default to noun if not found\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to remove stopwords, stem and lemmatize the text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Stem and lemmatize the filtered tokens\n",
        "    stem_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    lemma_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in filtered_tokens]\n",
        "    # Join the stemmed and lemmatized tokens back into a sentence\n",
        "    preprocessed_text = ' '.join(lemma_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply the preprocess_text function to the text column of the dataframe\n",
        "go_emotion_simplified_all['text'] = go_emotion_simplified_all['text'].apply(preprocess_text)\n",
        "\n",
        "# Print the first 10 rows of the cleaned text column\n",
        "print(go_emotion_simplified_all['text'][:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGJSmhnGV-xg",
        "outputId": "5c8560e1-92e3-4fe1-dadb-3850a231586e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define the 13 emotion labels\n",
        "emotions = ['happiness', 'sadness', 'fear', 'surprise', 'anger', 'disgust', 'approval',\n",
        "            'anticipation', 'realization','desire','shame','relief','love','neutral']\n",
        "\n",
        "# Create a label encoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to the emotion labels\n",
        "le.fit(emotions)\n",
        "\n",
        "# Transform the emotion labels to numerical values\n",
        "encoded_emotions = le.transform(emotions)\n",
        "\n",
        "# Print the encoded emotion labels\n",
        "print(encoded_emotions)\n",
        "\n",
        "# Print the emotion labels and the corresponding encoded values\n",
        "print(\"Emotion Labels:\")\n",
        "for label, encoded_value in zip(emotions, le.transform(emotions)):\n",
        "    print(label, \":\", encoded_value)\n",
        "\n",
        "# Print the encoder itself\n",
        "print(\"\\nLabelEncoder:\", le)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyufOkzIV-xh",
        "outputId": "b6975515-3f25-4932-b417-3880be9bf882"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r\"\\w+|$[0-9.]+|\\S+.>,<\")\n",
        "tokenizer.tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oZn4AF4V-xi",
        "outputId": "7b3b08cb-5714-4c1f-a2cd-bae38a539940"
      },
      "outputs": [],
      "source": [
        "# create a copy of the dataframe\n",
        "df_with_label = go_emotion_simplified_all.copy()\n",
        "\n",
        "# create a new column 'Label' and set its values based on the max value in each row\n",
        "df_with_label['label'] = df_with_label[['happiness', 'sadness', 'fear', 'surprise', 'anger', 'disgust', 'approval', 'anticipation', 'realization', 'desire', 'shame', 'relief', 'love', 'neutral']].apply(lambda x: x.idxmax(), axis=1)\n",
        "# concatenate dataframes\n",
        "go_emotion_simplified_all = pd.concat([go_emotion_simplified_all, df_with_label['label']], axis=1)\n",
        "# print the resulting dataframe\n",
        "print(go_emotion_simplified_all.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rigpbh0P0qpm",
        "outputId": "c8dc1fbb-b092-4a36-a7a3-3c03f7044477"
      },
      "outputs": [],
      "source": [
        "go_emotion_simplified_updated = go_emotion_simplified_all.drop(go_emotion_simplified_all.columns[1:15], axis=1)#keeping only necessary columns in the dataframe\n",
        "print(go_emotion_simplified_updated.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KfCAtC7WqNG"
      },
      "source": [
        "### CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p0s6qf0O9yP",
        "outputId": "0863810f-cced-4f79-cff6-9aa79d54b5bc"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow #installing tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coxLaPoOW14h"
      },
      "source": [
        "### CNN - Variation - 1 - Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P-2Y5ARdekaU",
        "outputId": "eb5cd5b2-170b-4cd8-a81d-64ce95b36237"
      },
      "outputs": [],
      "source": [
        "#CNN - 45% - without early stopping\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalMaxPooling1D, Conv1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "X = go_emotion_simplified_updated['text'].values\n",
        "y = go_emotion_simplified_updated['label'].values\n",
        "\n",
        "# Convert the labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
        "\n",
        "# Convert the emotion labels to one-hot encoded vectors\n",
        "num_classes = len(np.unique(y))\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (maxlen,)\n",
        "input_layer = Input(shape=input_shape)\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\n",
        "conv1d_layer_1 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "maxpooling1d_layer_1 = GlobalMaxPooling1D()(conv1d_layer_1)\n",
        "dropout_layer_1 = Dropout(rate=0.2)(maxpooling1d_layer_1)\n",
        "dense_layer_1 = Dense(units=64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(rate=0.2)(dense_layer_1)\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(dropout_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPUF-lzvrvJ4"
      },
      "source": [
        "## MisClassification Rate Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNKagJ_2b0l5",
        "outputId": "8b23e50b-0689-497c-c290-ca093de782ab"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the confusion matrix\n",
        "conf_matrix = np.array([[391, 10, 27, 28, 90, 28, 31, 17, 243, 2, 8, 12, 8, 34],\n",
        "                        [10, 173, 18, 46, 9, 8, 26, 18, 97, 2, 11, 7, 4, 21],\n",
        "                        [53, 21, 143, 67, 46, 28, 28, 20, 251, 6, 17, 1, 2, 35],\n",
        "                        [22, 43, 54, 468, 30, 15, 51, 27, 134, 8, 48, 11, 3, 35],\n",
        "                        [95, 14, 57, 40, 172, 37, 29, 11, 275, 5, 7, 33, 7, 56],\n",
        "                        [31, 9, 28, 7, 24, 139, 12, 10, 163, 6, 4, 10, 4, 31],\n",
        "                        [18, 36, 16, 38, 13, 3, 629, 15, 69, 6, 12, 1, 0, 12],\n",
        "                        [25, 18, 36, 43, 24, 16, 28, 241, 89, 4, 2, 14, 1, 31],\n",
        "                        [212, 87, 190, 159, 210, 123, 125, 70, 1725, 27, 16, 52, 6, 173],\n",
        "                        [8, 4, 12, 16, 11, 11, 6, 2, 65, 23, 5, 3, 2, 16],\n",
        "                        [9, 16, 16, 31, 8, 1, 28, 4, 20, 1, 386, 3, 3, 8],\n",
        "                        [25, 11, 11, 10, 29, 9, 9, 8, 65, 2, 2, 133, 19, 13],\n",
        "                        [20, 1, 4, 1, 8, 4, 5, 2, 17, 2, 1, 18, 45, 4],\n",
        "                        [37, 26, 32, 47, 34, 26, 25, 9, 236, 11, 14, 10, 2, 171]])\n",
        "\n",
        "# Calculate the misclassification rate for each class\n",
        "misclass_rate = 1 - np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
        "\n",
        "print(\"Misclassification rate for each class:\")\n",
        "for i, rate in enumerate(misclass_rate):\n",
        "    print(f\"Class {i}: {rate:.2%}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCJx2rFGW9-S"
      },
      "source": [
        "### CNN With Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4NK9p9yMCbq8",
        "outputId": "5f6bdfc6-7116-4117-9510-24164bdb0822"
      },
      "outputs": [],
      "source": [
        "#CNN  - 45% - 49.13% accuracy and 1.92% loss with early stopping\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalMaxPooling1D, Conv1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "X = go_emotion_simplified_updated['text'].values\n",
        "y = go_emotion_simplified_updated['label'].values\n",
        "\n",
        "# Convert the labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
        "\n",
        "# Convert the emotion labels to one-hot encoded vectors\n",
        "num_classes = len(np.unique(y))\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (maxlen,)\n",
        "input_layer = Input(shape=input_shape)\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\n",
        "conv1d_layer_1 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "maxpooling1d_layer_1 = GlobalMaxPooling1D()(conv1d_layer_1)\n",
        "dropout_layer_1 = Dropout(rate=0.2)(maxpooling1d_layer_1)\n",
        "dense_layer_1 = Dense(units=64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(rate=0.2)(dense_layer_1)\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(dropout_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks=[early_stop] )\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl9pcWMNXOlG"
      },
      "source": [
        "### CNN Variation 2 - Loss Function - KL Divergence Loss frunction from Categorical Cross Entropy Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OKVR6chXKbs1",
        "outputId": "3c20a51e-8a1a-472c-d10b-bf882136dcf7"
      },
      "outputs": [],
      "source": [
        "#CNN - final - 45% - 49.42 with early stopping and changed loss function to KLDivergence loss function variation 2/1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalMaxPooling1D, Conv1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "X = go_emotion_simplified_updated['text'].values\n",
        "y = go_emotion_simplified_updated['label'].values\n",
        "\n",
        "# Convert the labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
        "\n",
        "# Convert the emotion labels to one-hot encoded vectors\n",
        "num_classes = len(np.unique(y))\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (maxlen,)\n",
        "input_layer = Input(shape=input_shape)\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\n",
        "conv1d_layer_1 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "maxpooling1d_layer_1 = GlobalMaxPooling1D()(conv1d_layer_1)\n",
        "dropout_layer_1 = Dropout(rate=0.2)(maxpooling1d_layer_1)\n",
        "dense_layer_1 = Dense(units=64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(rate=0.2)(dense_layer_1)\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(dropout_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "#changing loss function to Kullback Leibler Divergence Loss function\n",
        "model.compile(optimizer='adam', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks=[early_stop] )\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y80uAzNyXpWn"
      },
      "source": [
        "## CNN Variation 2 - KL DIvergence Loss function and Optimizer changed to Adamax optmizer from Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rs_SJeNnqZ9P",
        "outputId": "7faa9853-240e-4ae7-cb05-73b1df70dbbd"
      },
      "outputs": [],
      "source": [
        "#CNN - final - 45% - 49.42-49.42-53 with early stopping and changed loss function to KLDivergence loss function  and adamax optimizer from adam variation 2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalMaxPooling1D, Conv1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "X = go_emotion_simplified_updated['text'].values\n",
        "y = go_emotion_simplified_updated['label'].values\n",
        "\n",
        "# Convert the labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
        "\n",
        "# Convert the emotion labels to one-hot encoded vectors\n",
        "num_classes = len(np.unique(y))\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (maxlen,)\n",
        "input_layer = Input(shape=input_shape)\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\n",
        "conv1d_layer_1 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "maxpooling1d_layer_1 = GlobalMaxPooling1D()(conv1d_layer_1)\n",
        "dropout_layer_1 = Dropout(rate=0.2)(maxpooling1d_layer_1)\n",
        "dense_layer_1 = Dense(units=64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(rate=0.2)(dense_layer_1)\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(dropout_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adamax', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks=[early_stop] )\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKKhggpaswJP"
      },
      "source": [
        "MisClassification Rate Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JLW05fErG5b",
        "outputId": "5f9295c8-ff11-4b5f-8e6a-ea9b29cd1294"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the confusion matrix\n",
        "conf_matrix = np.array([[456, 4, 7, 13, 71, 8, 16, 16, 289, 0, 15, 16, 6, 12],\n",
        "                             [7, 201, 5, 38, 6, 3, 19, 13, 121, 0, 12, 0, 3, 22],\n",
        "                             [29, 14, 105, 69, 31, 13, 29, 31, 365, 0, 17, 4, 1, 10],\n",
        "                             [16, 24, 14, 534, 23, 8, 46, 37, 170, 0, 50, 8, 3, 16],\n",
        "                             [82, 14, 20, 43, 204, 16, 23, 14, 371, 1, 4, 22, 8, 16],\n",
        "                             [23, 6, 8, 15, 66, 118, 8, 10, 191, 0, 4, 9, 4, 16],\n",
        "                             [14, 15, 7, 34, 4, 0, 674, 17, 75, 0, 16, 2, 0, 10],\n",
        "                             [14, 17, 9, 32, 13, 5, 17, 338, 112, 0, 1, 5, 2, 7],\n",
        "                             [164, 28, 78, 114, 179, 32, 88, 75, 2306, 0, 11, 33, 9, 58],\n",
        "                             [11, 1, 11, 17, 10, 9, 6, 5, 84, 0, 7, 3, 9, 11],\n",
        "                             [4, 12, 5, 29, 4, 0, 31, 5, 18, 0, 422, 1, 1, 2],\n",
        "                             [15, 8, 4, 8, 41, 3, 9, 4, 83, 0, 3, 125, 39, 4],\n",
        "                             [4, 2, 1, 1, 12, 2, 2, 1, 17, 0, 3, 20, 65, 2],\n",
        "                             [22, 12, 8, 39, 31, 7, 22, 16, 307, 0, 16, 8, 7, 185]])\n",
        "\n",
        "# Calculate the misclassification rate for each class\n",
        "misclass_rate = 1 - np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
        "\n",
        "print(\"Misclassification rate for each class:\")\n",
        "for i, rate in enumerate(misclass_rate):\n",
        "    print(f\"Class {i}: {rate:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlMObl4YMvr"
      },
      "source": [
        "# CNN Variation - 3 - Hyper parameter tuning - batch_size = 64 to 32, epochs = 10 to 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3cC2031iVxGC",
        "outputId": "5aebd741-07ef-4342-bd2d-5e71edaabb97"
      },
      "outputs": [],
      "source": [
        "#CNN - final - 45% - 49.42-49.42-54 with early stopping and changed loss function to KLDivergence loss function  and adamax optimizer from adam variation 2\n",
        "#Variation 3 - hyper parameter tuning - batch_size = 64 to 32, epochs = 10 to 20 - 53% accuracy - 1.65% test loss\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalMaxPooling1D, Conv1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "X = go_emotion_simplified_updated['text'].values\n",
        "y = go_emotion_simplified_updated['label'].values\n",
        "\n",
        "# Convert the labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
        "\n",
        "# Convert the emotion labels to one-hot encoded vectors\n",
        "num_classes = len(np.unique(y))\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (maxlen,)\n",
        "input_layer = Input(shape=input_shape)\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\n",
        "conv1d_layer_1 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "maxpooling1d_layer_1 = GlobalMaxPooling1D()(conv1d_layer_1)\n",
        "dropout_layer_1 = Dropout(rate=0.2)(maxpooling1d_layer_1)\n",
        "dense_layer_1 = Dense(units=64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(rate=0.2)(dense_layer_1)\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(dropout_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adamax', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32 # Batch size changed \n",
        "epochs = 20 #epoch changed\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), callbacks=[early_stop])\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzm8ef_32qU1",
        "outputId": "9a1fd81f-8ac2-4c90-828c-af4efadef7c6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the confusion matrix\n",
        "conf_matrix = np.array([[ 423,5,16,11,74,15,22,18,296,3,17,9,7,13],\n",
        " [   6,194,10,37,8,4,25,14,107,0,13,3,2,27],\n",
        " [  33,18,124,70,28,14,24,30,338,0,18,4,1,16],\n",
        " [  17,23,28,529,7,8,47,38,176,5,49,5,1,16],\n",
        " [  79,14,31,32,193,10,31,14,381,5,5,16,7,20],\n",
        " [  37,5,13,10,28,136,12,5,197,1,4,6,4,20],\n",
        " [  13,15,7,34,4,2,682,17,64,1,14,2,0,13],\n",
        " [  15,19,9,31,13,3,22,331,110,0,1,8,2,8],\n",
        " [ 190,31,97,138,125,37,110,69,2255,5,10,23,8,77],\n",
        " [  11,2,8,16,7,5,6,4,85,17,9,4,2,8],\n",
        " [   2,11,2,33,5,0,27,4,20,2,420,2,1,5],\n",
        " [  13,7,3,9,31,3,9,5,89,0,4,135,33,5],\n",
        " [   6,1,3,1,18,0,3,2,18,0,3,17,59,1],\n",
        " [  28,12,13,39,19,5,25,10,304,4,14,8,5,194]])\n",
        "\n",
        "# Calculate the misclassification rate for each class\n",
        "misclass_rate = 1 - np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
        "\n",
        "print(\"Misclassification rate for each class:\")\n",
        "for i, rate in enumerate(misclass_rate):\n",
        "    print(f\"Class {i}: {rate:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzCnkJN4Yi-2"
      },
      "source": [
        "# CNN - Variation 4 - Without One hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qDZ5sMimdLR5",
        "outputId": "d200feca-1dae-447f-ba16-4f05fd9acb2e"
      },
      "outputs": [],
      "source": [
        "#CNN - final - 45% - 49.42-49.42-54 with early stopping and changed loss function to KLDivergence loss function  and adamax optimizer from adam variation 2\n",
        "#Variation 2 - hyper parameter tuning - batch_size = 64 to 32, epochs = 10 to 20 - 53% accuracy - 1.65% test loss\n",
        "#Variation - 3Text featurisation - without one hot encoding accuracy -3.18%, loss - 33%\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalMaxPooling1D, Conv1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "X = go_emotion_simplified_updated['text'].values\n",
        "y = go_emotion_simplified_updated['label'].values\n",
        "\n",
        "# Convert the labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (maxlen,)\n",
        "input_layer = Input(shape=input_shape)\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\n",
        "conv1d_layer_1 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "maxpooling1d_layer_1 = GlobalMaxPooling1D()(conv1d_layer_1)\n",
        "dropout_layer_1 = Dropout(rate=0.2)(maxpooling1d_layer_1)\n",
        "dense_layer_1 = Dense(units=64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(rate=0.2)(dense_layer_1)\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(dropout_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adamax', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks=[early_stop] )\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tegt7s-_YzbZ"
      },
      "source": [
        "# CNN Variation - 5 - Change train validation test split into 70/30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6DE2DStxgDNw",
        "outputId": "e8ef44dd-677a-489a-af32-e3578aee26b2"
      },
      "outputs": [],
      "source": [
        "#CNN - final - 45% - 49.42%-49.42%-54% with early stopping and changed loss function to KLDivergence loss function  and adamax optimizer from adam variation 2\n",
        "#Variation - 4 Change train validation test split into 70/30 - 53% accuracy and 1.64% loss\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalMaxPooling1D, Conv1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "X = go_emotion_simplified_updated['text'].values\n",
        "y = go_emotion_simplified_updated['label'].values\n",
        "\n",
        "# Convert the labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
        "\n",
        "# Convert the emotion labels to one-hot encoded vectors\n",
        "num_classes = len(np.unique(y))\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (maxlen,)\n",
        "input_layer = Input(shape=input_shape)\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\n",
        "conv1d_layer_1 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "maxpooling1d_layer_1 = GlobalMaxPooling1D()(conv1d_layer_1)\n",
        "dropout_layer_1 = Dropout(rate=0.2)(maxpooling1d_layer_1)\n",
        "dense_layer_1 = Dense(units=64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(rate=0.2)(dense_layer_1)\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(dropout_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adamax', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks=[early_stop] )\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP12GSgMZoWT"
      },
      "source": [
        "# Some Hyper Parameter Tuning for accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EM6cuQpT9nnF",
        "outputId": "fe535d42-8c50-4083-b387-bbd1db982616"
      },
      "outputs": [],
      "source": [
        "#CNN - final - 45% - 49.42-49.42-54 with early stopping and changed loss function to KLDivergence loss function  and adamax optimizer from adam variation 2\n",
        "#batch size - 128 and train val test split -0.1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalMaxPooling1D, Conv1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import precision_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "X = go_emotion_simplified_updated['text'].values\n",
        "y = go_emotion_simplified_updated['label'].values\n",
        "\n",
        "# Convert the labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
        "\n",
        "# Convert the emotion labels to one-hot encoded vectors\n",
        "num_classes = len(np.unique(y))\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (maxlen,)\n",
        "input_layer = Input(shape=input_shape)\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\n",
        "conv1d_layer_1 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "maxpooling1d_layer_1 = GlobalMaxPooling1D()(conv1d_layer_1)\n",
        "dropout_layer_1 = Dropout(rate=0.2)(maxpooling1d_layer_1)\n",
        "dense_layer_1 = Dense(units=64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(rate=0.2)(dense_layer_1)\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(dropout_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adamax', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks=[early_stop] )\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='macro', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='macro', zero_division=1)\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51iWDoZShDsj"
      },
      "source": [
        "### LSTM - Variation 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AQ9k6E3Rjbtu",
        "outputId": "9e64b5b6-55a4-47e0-db47-db4712fcd79e"
      },
      "outputs": [],
      "source": [
        "#LSTM - accuracy 42%\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "# Filter the data to include only the relevant columns and labels\n",
        "go_emotion_simplified_updated = go_emotion_simplified_updated[['text', 'label']]\n",
        "\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(go_emotion_simplified_updated, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(train_data['label'])\n",
        "\n",
        "# Tokenize the text data and pad the sequences to a fixed length\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "max_sequence_length = 100\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['text'])\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create the model\n",
        "embedding_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_size, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(units=64)))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=len(le.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(train_sequences, to_categorical(le.transform(train_data['label'])), epochs=epochs, batch_size=batch_size, validation_data=(test_sequences, to_categorical(le.transform(test_data['label']))))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_sequences, to_categorical(le.transform(test_data['label'])))\n",
        "print('Test accuracy:', test_acc)\n",
        "print(\"Test loss:\", test_loss)\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_true = le.transform(test_data['label'])\n",
        "y_pred = model.predict(test_sequences).argmax(axis=-1)\n",
        "#y_pred = model.predict_classes(test_sequences)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy and loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ8m5rS8hMlY"
      },
      "source": [
        "## LSTM - Variation 2 - One Hot Encoding Used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QzViPXKk6yCI",
        "outputId": "92f1c3be-6c32-48f2-8370-adec4dcc7504"
      },
      "outputs": [],
      "source": [
        "#LSTM - accuracy 42% - one hot encoding used - variation 1 LSTM in datapreprocessing - 41% after variation\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Filter the data to include only the relevant columns and labels\n",
        "go_emotion_simplified_updated = go_emotion_simplified_updated[['text', 'label']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(go_emotion_simplified_updated, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(train_data['label'])\n",
        "\n",
        "# Tokenize the text data and pad the sequences to a fixed length\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "max_sequence_length = 100\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['text'])\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "train_labels = le.transform(train_data['label'])\n",
        "test_labels = le.transform(test_data['label'])\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "num_classes = len(set(train_data['label']))\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
        "\n",
        "\n",
        "\n",
        "# Create the model\n",
        "embedding_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_size, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(units=64)))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=len(le.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(train_sequences, to_categorical(le.transform(train_data['label'])), epochs=epochs, batch_size=batch_size, validation_data=(test_sequences, to_categorical(le.transform(test_data['label']))))\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_sequences, to_categorical(le.transform(test_data['label'])))\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "print(\"Test loss:\", test_loss)\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_true = le.transform(test_data['label'])\n",
        "y_pred = model.predict(test_sequences).argmax(axis=-1)\n",
        "#y_pred = model.predict_classes(test_sequences)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy and loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdgL44DBgYaT"
      },
      "source": [
        "# LSTM - Variation -3 - 70/30 Train test split from 80/20 - 42% accuracy, 3.28% loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OwvEXXw6o_Gg",
        "outputId": "4f0fdb97-caa7-40db-b5d5-a1f7264e1d7b"
      },
      "outputs": [],
      "source": [
        "#LSTM - accuracy 43% - change label encoding to one hot encoding - variation 2 LSTM in datapreprocessing - 43% after variation\n",
        "#Variation 3 - 70/30 Train test split from 80/20 - 42% accuracy, 3.28% loss\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Filter the data to include only the relevant columns and labels\n",
        "go_emotion_simplified_updated = go_emotion_simplified_updated[['text', 'label']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(go_emotion_simplified_updated, test_size=0.3, random_state=42)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(train_data['label'])\n",
        "\n",
        "# Tokenize the text data and pad the sequences to a fixed length\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "max_sequence_length = 100\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['text'])\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "train_labels = le.transform(train_data['label'])\n",
        "test_labels = le.transform(test_data['label'])\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "num_classes = len(set(train_data['label']))\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
        "\n",
        "\n",
        "\n",
        "# Create the model\n",
        "embedding_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_size, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(units=64)))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=len(le.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(train_sequences, to_categorical(le.transform(train_data['label'])), epochs=epochs, batch_size=batch_size, validation_data=(test_sequences, to_categorical(le.transform(test_data['label']))))\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_sequences, to_categorical(le.transform(test_data['label'])))\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "print(\"Test loss:\", test_loss)\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_true = le.transform(test_data['label'])\n",
        "y_pred = model.predict(test_sequences).argmax(axis=-1)\n",
        "#y_pred = model.predict_classes(test_sequences)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy and loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXZ00nl7h0eZ"
      },
      "source": [
        "## LSTM Variation - 4 - #Optimizer - Adamax and Loss frunction KL Divergence Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y-VM3op1w1rd",
        "outputId": "d73b8a75-9b21-41c3-ecdc-c382b6da0284"
      },
      "outputs": [],
      "source": [
        "#LSTM - accuracy 43% - change label encoding to one hot encoding - variation 1 LSTM in datapreprocessing - 43% after variation\n",
        "#Optimizer - Adamax and Loss frunction KL Divergence Loss function\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Filter the data to include only the relevant columns and labels\n",
        "go_emotion_simplified_updated = go_emotion_simplified_updated[['text', 'label']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(go_emotion_simplified_updated, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(train_data['label'])\n",
        "\n",
        "# Tokenize the text data and pad the sequences to a fixed length\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "max_sequence_length = 100\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['text'])\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "train_labels = le.transform(train_data['label'])\n",
        "test_labels = le.transform(test_data['label'])\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "num_classes = len(set(train_data['label']))\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create the model\n",
        "embedding_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_size, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(units=64)))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=len(le.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adamax', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(train_sequences,\n",
        "                    to_categorical(le.transform(train_data['label'])),\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(test_sequences, to_categorical(le.transform(test_data['label']))))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_sequences, to_categorical(le.transform(test_data['label'])))\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "print(\"Test loss:\", test_loss)\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_true = le.transform(test_data['label'])\n",
        "y_pred = model.predict(test_sequences).argmax(axis=-1)\n",
        "#y_pred = model.predict_classes(test_sequences)\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# y_true is the true labels and y_pred is the predicted labels\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, zero_division=1)\n",
        "\n",
        "# print the precision, recall, and F-score for each class\n",
        "for i in range(num_classes):\n",
        "    print(f\"Class {i}: precision={precision[i]}, recall={recall[i]}, F-score={fscore[i]}\")\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy and loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZfR1CBHwq5zb",
        "outputId": "0cc5edc0-98e6-4a2e-a9da-bf28f1558d4c"
      },
      "outputs": [],
      "source": [
        "#LSTM - accuracy 43% - change label encoding to one hot encoding - variation 1 LSTM in datapreprocessing - 43% after variation\n",
        "#Optimizer - Adamax and Loss frunction KL Divergence Loss function\n",
        "Test size - 0.1\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Filter the data to include only the relevant columns and labels\n",
        "go_emotion_simplified_updated = go_emotion_simplified_updated[['text', 'label']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(go_emotion_simplified_updated, test_size=0.1, random_state=42)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(train_data['label'])\n",
        "\n",
        "# Tokenize the text data and pad the sequences to a fixed length\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "max_sequence_length = 100\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['text'])\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "train_labels = le.transform(train_data['label'])\n",
        "test_labels = le.transform(test_data['label'])\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "num_classes = len(set(train_data['label']))\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create the model\n",
        "embedding_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_size, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(units=64)))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=len(le.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adamax', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(train_sequences,\n",
        "                    to_categorical(le.transform(train_data['label'])),\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(test_sequences, to_categorical(le.transform(test_data['label']))))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_sequences, to_categorical(le.transform(test_data['label'])))\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "print(\"Test loss:\", test_loss)\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_true = le.transform(test_data['label'])\n",
        "y_pred = model.predict(test_sequences).argmax(axis=-1)\n",
        "#y_pred = model.predict_classes(test_sequences)\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# y_true is the true labels and y_pred is the predicted labels\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, zero_division=1)\n",
        "\n",
        "# print the precision, recall, and F-score for each class\n",
        "for i in range(num_classes):\n",
        "    print(f\"Class {i}: precision={precision[i]}, recall={recall[i]}, F-score={fscore[i]}\")\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy and loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uSB4jMvLrJzI",
        "outputId": "0d7cca65-d517-44ed-9a2e-16cb8942cc0f"
      },
      "outputs": [],
      "source": [
        "#LSTM - accuracy 43% - change label encoding to one hot encoding - variation 1 LSTM in datapreprocessing - 43% after variation\n",
        "#Optimizer - Adamax and Loss frunction KL Divergence Loss function\n",
        "#test size - 0.1\n",
        "#Batch size - 128\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Filter the data to include only the relevant columns and labels\n",
        "go_emotion_simplified_updated = go_emotion_simplified_updated[['text', 'label']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(go_emotion_simplified_updated, test_size=0.1, random_state=42)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(train_data['label'])\n",
        "\n",
        "# Tokenize the text data and pad the sequences to a fixed length\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "max_sequence_length = 100\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['text'])\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "train_labels = le.transform(train_data['label'])\n",
        "test_labels = le.transform(test_data['label'])\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "num_classes = len(set(train_data['label']))\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create the model\n",
        "embedding_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_size, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(units=64)))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=len(le.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adamax', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "history = model.fit(train_sequences,\n",
        "                    to_categorical(le.transform(train_data['label'])),\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(test_sequences, to_categorical(le.transform(test_data['label']))))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_sequences, to_categorical(le.transform(test_data['label'])))\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "print(\"Test loss:\", test_loss)\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_true = le.transform(test_data['label'])\n",
        "y_pred = model.predict(test_sequences).argmax(axis=-1)\n",
        "#y_pred = model.predict_classes(test_sequences)\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# y_true is the true labels and y_pred is the predicted labels\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, zero_division=1)\n",
        "\n",
        "# print the precision, recall, and F-score for each class\n",
        "for i in range(num_classes):\n",
        "    print(f\"Class {i}: precision={precision[i]}, recall={recall[i]}, F-score={fscore[i]}\")\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy and loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E610FFCyHEs"
      },
      "source": [
        "## **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HMoz1GPmW2w",
        "outputId": "69675fd5-6cad-46bb-c3e2-b09ad875e2b2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(go_emotion_simplified_all['text'], go_emotion_simplified_all['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data using the CountVectorizer\n",
        "stop_words = {'a', 'an', 'the', ...}  # Set of stop words\n",
        "stop_words_list = list(stop_words)   # Convert set to list\n",
        "vectorizer = CountVectorizer(stop_words=stop_words_list)\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the SVM model\n",
        "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
